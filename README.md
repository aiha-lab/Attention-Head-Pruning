# Attention-Head-Pruning
Layer-wise Pruning of Transformer Heads for Efficient Language Modeling
